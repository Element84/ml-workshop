{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50ec38a8-c16e-4eed-b7e5-e91fb14ef1c0",
   "metadata": {},
   "source": [
    "Vision-language models\n",
    "===\n",
    "\n",
    "In this notebook we use SkyCLIP, a vision-language model trained on Earth imagery, for text-to-image retrieval and zero-shot classification. \n",
    "\n",
    "See [this blog post](https://element84.com/machine-learning/towards-a-queryable-earth-with-vision-language-foundation-models/) on how the text-to-image retrieval functionality can be scaled over large regions to make them queryable with natural language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca66bc83-075b-4170-8b0a-8bc2d14a208a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env AWS_REQUEST_PAYER=requester"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84508ce0-3d3d-4a07-bf37-93dc60f42a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rastervision.pipeline.file_system.utils import (download_or_copy,\n",
    "                                                     list_paths)\n",
    "from rastervision.core.box import Box\n",
    "from rastervision.pytorch_learner.dataset import (\n",
    "    SemanticSegmentationSlidingWindowGeoDataset)\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "from torch.utils.data import ConcatDataset, DataLoader\n",
    "from shapely.geometry import mapping\n",
    "import pystac_client\n",
    "import geopandas as gpd\n",
    "import contextily as cx\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.reset_defaults()\n",
    "\n",
    "DEVICE = 'cuda'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e39ebfe-4b6b-47c6-9cb2-33c6e1890661",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bef7a11-83f1-4fac-8976-7930a28dca77",
   "metadata": {},
   "source": [
    "# Load SkyCLIP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f17b663-7920-4065-a28e-af5a890f926f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp s3://ml-workshop-internal/vlm/skyclip.pt data/skyclip.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f8d0a1-3c63-459c-8829-dfbfaef87011",
   "metadata": {},
   "outputs": [],
   "source": [
    "import open_clip\n",
    "\n",
    "model_name = 'ViT-L-14'\n",
    "model, _, preprocess = open_clip.create_model_and_transforms(model_name)\n",
    "tokenizer = open_clip.get_tokenizer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880e20a9-a366-4406-b7c1-2a148a42bbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = 'data/skyclip.pt'\n",
    "ckpt = torch.load(ckpt_path, map_location=DEVICE)['state_dict']\n",
    "ckpt = {k[len('module.'):]: v for k, v in ckpt.items()}\n",
    "message = model.load_state_dict(ckpt)\n",
    "model = model.cuda().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acedba09-7967-4c53-9c18-f3d94b865b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('#params: ', f'{sum(p.numel() for p in model.parameters()):,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8aab304-95bb-4abc-838d-d65b055ca8df",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1decbe3-2443-4c26-abc6-982423521596",
   "metadata": {},
   "source": [
    "# Get imagery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f897b1bc-104b-4bf7-97ad-6a72f26c0e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox = Box(ymin=39.889060, xmin=-75.104968, ymax=39.989442, xmax=-75.246207)\n",
    "bbox_polygon = bbox.to_shapely().oriented_envelope\n",
    "search_geometry = mapping(bbox_polygon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1712d93-41e4-40c6-ae05-15efae11ad1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog = pystac_client.Client.open(\n",
    "    'https://earth-search.aws.element84.com/v1')\n",
    "\n",
    "items = catalog.search(\n",
    "    intersects=search_geometry,\n",
    "    datetime='2019-10-01/2020-01-01',\n",
    "    collections=['naip'],\n",
    ").item_collection()\n",
    "len(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15b743f-c09a-4e1d-b0aa-854c98e13c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gpd.GeoDataFrame.from_features(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0584a245-56e7-4a0f-a7dc-792f66bece13",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "gdf.plot(ax=ax, ec='k', fc='none')\n",
    "cx.add_basemap(ax, crs='epsg:4326', source=cx.providers.CartoDB.Voyager)\n",
    "ax.set_xlabel('longitude')\n",
    "ax.set_ylabel('latitude')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51504e47-83af-41b8-8189-170174ba9e3a",
   "metadata": {},
   "source": [
    "# Generate vector embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2123bb44-6d70-41d7-b36a-b04528911770",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for item in items:\n",
    "    download_or_copy(item.assets['image'].href, 'data/naip', delete_tmp=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5166c9bb-911b-49f7-8229-13e7d2a8a87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_uris = list_paths('data/naip', ext='tif')\n",
    "img_uris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cd82c5-c505-4c3d-8469-6e72bee13e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dses = [None] * len(img_uris)\n",
    "for i, uri in enumerate(img_uris):\n",
    "    dses[i] = SemanticSegmentationSlidingWindowGeoDataset.from_uris(\n",
    "        image_uri=uri,\n",
    "        image_raster_source_kw=dict(channel_order=[0, 1, 2]),\n",
    "        size=400,\n",
    "        stride=400,\n",
    "        out_size=224,\n",
    "    )\n",
    "ds = ConcatDataset(dses)\n",
    "len(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7081a58-5646-4edc-8e8d-def8857b6c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DataLoader(ds, batch_size=16, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c159264-7ec2-4378-92cb-a8e949ae8b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM_SIZE = 768\n",
    "embs = torch.zeros(len(ds), EMBEDDING_DIM_SIZE)\n",
    "\n",
    "with torch.inference_mode(), tqdm(dl, desc='Creating chip embeddings') as bar:\n",
    "    i = 0\n",
    "    for x, _ in bar:\n",
    "        x = x.to(DEVICE)\n",
    "        emb = model.encode_image(x)\n",
    "        embs[i:i + len(x)] = emb.cpu()\n",
    "        i += len(x)\n",
    "\n",
    "# normalize the embeddings\n",
    "embs /= embs.norm(dim=-1, keepdim=True)\n",
    "\n",
    "embs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71fb91b-f061-4608-9a7f-294e45d50f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "embs_path = 'data/skyclip_naip_embeddings.pt'\n",
    "torch.save(embs, embs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61886a6-a522-403f-bdcc-8794c7d995f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !aws s3 cp {embs_path} s3://ml-workshop-internal/2024_05_02/<YOUR NAME>/skyclip_naip_embeddings.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb61ce0-cf68-4de1-9428-1d2836009bed",
   "metadata": {},
   "source": [
    "# Text-to-image retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd130cb-473a-409a-bda0-51e1c3ce4bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "windows = sum((_ds.windows for _ds in ds.datasets), [])\n",
    "len(windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df02276c-fc85-4c2e-bf10-a9093ef23195",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chip_scores(text_queries, embs):\n",
    "    assert len(text_queries) == 1\n",
    "    text = tokenizer(text_queries)\n",
    "    with torch.inference_mode():\n",
    "        text_features = model.encode_text(text.to(DEVICE))\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "        text_features = text_features.cpu()\n",
    "        chip_scores = torch.cosine_similarity(text_features, embs)\n",
    "    return chip_scores\n",
    "\n",
    "\n",
    "def emb_idx_to_chip(i, windows, out_shape=(400, 400)):\n",
    "    chip, _ = ds[int(i)]\n",
    "    chip = chip.permute(1, 2, 0)\n",
    "    return chip\n",
    "\n",
    "\n",
    "def show_top_chips(chip_scores,\n",
    "                   windows_df,\n",
    "                   top_inds=None,\n",
    "                   nrows=5,\n",
    "                   ncols=5,\n",
    "                   figsize=(12, 12),\n",
    "                   w_pad=-2.5,\n",
    "                   h_pad=-2.5):\n",
    "    plt.close('all')\n",
    "    fig, axs = plt.subplots(nrows, ncols, figsize=figsize)\n",
    "    fig.tight_layout(w_pad=w_pad, h_pad=h_pad)\n",
    "    if top_inds is None:\n",
    "        top_inds = torch.topk(chip_scores, axs.size).indices\n",
    "    for ax, i in zip(tqdm(axs.flat), top_inds):\n",
    "        chip = emb_idx_to_chip(i, windows_df)\n",
    "        ax.imshow(chip)\n",
    "    for ax in axs.flat:\n",
    "        ax.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309304ad-f209-40fa-9246-37157d2b4597",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_query = 'stadium'\n",
    "\n",
    "chip_scores = get_chip_scores([text_query], embs)\n",
    "show_top_chips(chip_scores,\n",
    "               windows,\n",
    "               nrows=2,\n",
    "               ncols=4,\n",
    "               figsize=(12, 6),\n",
    "               w_pad=-(12 / 4),\n",
    "               h_pad=-(6 / 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce618429-6e23-4cb2-bd79-29b61d73f004",
   "metadata": {},
   "source": [
    "# Zero-shot classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55084987-bef4-4b5b-b7f0-57a996b2016d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_scores(text_queries, embs, T):\n",
    "    assert len(embs) == 1\n",
    "    text = tokenizer(text_queries)\n",
    "    with torch.inference_mode():\n",
    "        text_features = model.encode_text(text.to(DEVICE))\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "        text_features = text_features.cpu()\n",
    "        out = (embs @ text_features.T)\n",
    "        out = (out / T).softmax(dim=1)\n",
    "        out = out.numpy().squeeze()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37b720a-359f-47a5-bcb8-9ab0f46b19cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "img, _ = ds[3583]\n",
    "img_emb = embs[[3583]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581888af-eb3b-4da7-ad43-0d7107760c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img.permute(1, 2, 0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80301713-872b-4fc5-a60a-292fcc50d90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\n",
    "    'forest',\n",
    "    'harbor',\n",
    "    'stadium',\n",
    "    'parking lot',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5733509e-5794-4c54-8fbc-a500d8c24890",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_scores = get_text_scores(classes, img_emb, T=0.05)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(classes, text_scores, ec='black')\n",
    "ax.set_ylim((0, 1))\n",
    "ax.yaxis.grid(linestyle='--', alpha=1)\n",
    "ax.set_xlabel('Class')\n",
    "ax.set_ylabel('Probability')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
